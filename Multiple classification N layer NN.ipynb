{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea025264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbbba86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    A = np.maximum(0, Z)\n",
    "    activation_cache = Z\n",
    "    return A, activation_cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88b8929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    activation_cache = Z\n",
    "    return A, activation_cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7668aee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    np.random.seed(1)\n",
    "\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "\n",
    "    parameters = {\n",
    "        \"W1\": W1,\n",
    "        \"b1\": b1,\n",
    "        \"W2\": W2,\n",
    "        \"b2\": b2\n",
    "    }\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d311958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)  # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1])  # *0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b39dc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "\n",
    "    return Z, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e992767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    elif activation == \"tanh\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = tanh(Z)\n",
    "    else:\n",
    "        print(\"Invalid activation function\")\n",
    "        return\n",
    "\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4a51e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> ACTIVATION]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], activation=\"relu\")\n",
    "        caches.append(cache)\n",
    "\n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], activation=\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5caff9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y,epsilon=1e-8):\n",
    "    m = Y.shape[1]\n",
    "    cost = -(1/m) * np.sum(Y * np.log(AL + epsilon))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "255906d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return dA_prev, dW, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3edec2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, activation_cache):\n",
    "    Z = activation_cache\n",
    "    s = 1 / (1 + np.exp(-Z))\n",
    "    dZ = dA * s * (1 - s)\n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b45234ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, activation_cache):\n",
    "    Z = activation_cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97271af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_backward(dA, activation_cache):\n",
    "    Z = activation_cache\n",
    "    s = np.tanh(Z)\n",
    "    dZ = dA * (1 - s**2)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f38faeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    elif activation == \"tanh\":\n",
    "        dZ = tanh_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    else:\n",
    "        print(\"Invalid activation function\")\n",
    "        return\n",
    "\n",
    "    return dA_prev, dW, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f759927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches, epsilon=1e-8):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    dAL = - (np.divide(Y, AL + epsilon) - np.divide(1 - Y, 1 - AL + epsilon))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation=\"sigmoid\")\n",
    "\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, activation=\"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3f7b761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(params, grads, learning_rate):\n",
    "    L = len(params) // 2  # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter\n",
    "    for l in range(L):\n",
    "        params[\"W\" + str(l+1)] = params[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        params[\"b\" + str(l+1)] = params[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7eea22fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False):\n",
    "    np.random.seed(1)\n",
    "    costs = []  # keep track of cost\n",
    "\n",
    "    # Parameters initialization.\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "\n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "\n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "\n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "#         print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 10 == 0:\n",
    "            print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "\n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters,costs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9768e944",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/mandeez/Downloads/Updated_Dataset.csv\",encoding='ISO-8859-1',delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b665a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bitki_Türü</th>\n",
       "      <th>Tarih</th>\n",
       "      <th>Hour</th>\n",
       "      <th>380_440</th>\n",
       "      <th>400_460</th>\n",
       "      <th>420_480</th>\n",
       "      <th>460_520</th>\n",
       "      <th>480_540</th>\n",
       "      <th>510_570</th>\n",
       "      <th>530_590</th>\n",
       "      <th>...</th>\n",
       "      <th>670_730</th>\n",
       "      <th>700_760</th>\n",
       "      <th>730_790</th>\n",
       "      <th>780_840</th>\n",
       "      <th>820_880</th>\n",
       "      <th>860_920</th>\n",
       "      <th>900_960</th>\n",
       "      <th>SICAKLIK</th>\n",
       "      <th>NEM</th>\n",
       "      <th>Fruit Numeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Topalak</td>\n",
       "      <td>43578</td>\n",
       "      <td>0.710417</td>\n",
       "      <td>424</td>\n",
       "      <td>431</td>\n",
       "      <td>443</td>\n",
       "      <td>486</td>\n",
       "      <td>517</td>\n",
       "      <td>543</td>\n",
       "      <td>563</td>\n",
       "      <td>...</td>\n",
       "      <td>690</td>\n",
       "      <td>734</td>\n",
       "      <td>789</td>\n",
       "      <td>829</td>\n",
       "      <td>839</td>\n",
       "      <td>888</td>\n",
       "      <td>939</td>\n",
       "      <td>24</td>\n",
       "      <td>163</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Topalak</td>\n",
       "      <td>43467</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>404</td>\n",
       "      <td>442</td>\n",
       "      <td>451</td>\n",
       "      <td>483</td>\n",
       "      <td>514</td>\n",
       "      <td>526</td>\n",
       "      <td>552</td>\n",
       "      <td>...</td>\n",
       "      <td>685</td>\n",
       "      <td>736</td>\n",
       "      <td>783</td>\n",
       "      <td>829</td>\n",
       "      <td>844</td>\n",
       "      <td>897</td>\n",
       "      <td>939</td>\n",
       "      <td>20</td>\n",
       "      <td>176</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ayrik_Otu</td>\n",
       "      <td>43721</td>\n",
       "      <td>0.831250</td>\n",
       "      <td>383</td>\n",
       "      <td>409</td>\n",
       "      <td>446</td>\n",
       "      <td>507</td>\n",
       "      <td>495</td>\n",
       "      <td>535</td>\n",
       "      <td>575</td>\n",
       "      <td>...</td>\n",
       "      <td>708</td>\n",
       "      <td>722</td>\n",
       "      <td>749</td>\n",
       "      <td>785</td>\n",
       "      <td>875</td>\n",
       "      <td>916</td>\n",
       "      <td>941</td>\n",
       "      <td>20</td>\n",
       "      <td>169</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kanyas</td>\n",
       "      <td>43697</td>\n",
       "      <td>0.952083</td>\n",
       "      <td>408</td>\n",
       "      <td>431</td>\n",
       "      <td>447</td>\n",
       "      <td>471</td>\n",
       "      <td>502</td>\n",
       "      <td>513</td>\n",
       "      <td>545</td>\n",
       "      <td>...</td>\n",
       "      <td>696</td>\n",
       "      <td>712</td>\n",
       "      <td>774</td>\n",
       "      <td>812</td>\n",
       "      <td>872</td>\n",
       "      <td>838</td>\n",
       "      <td>932</td>\n",
       "      <td>25</td>\n",
       "      <td>96</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sirken</td>\n",
       "      <td>43629</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>403</td>\n",
       "      <td>423</td>\n",
       "      <td>440</td>\n",
       "      <td>495</td>\n",
       "      <td>515</td>\n",
       "      <td>541</td>\n",
       "      <td>556</td>\n",
       "      <td>...</td>\n",
       "      <td>699</td>\n",
       "      <td>735</td>\n",
       "      <td>743</td>\n",
       "      <td>805</td>\n",
       "      <td>853</td>\n",
       "      <td>889</td>\n",
       "      <td>954</td>\n",
       "      <td>26</td>\n",
       "      <td>107</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Bitki_Türü  Tarih      Hour  380_440  400_460  420_480  460_520  480_540  \\\n",
       "0    Topalak  43578  0.710417      424      431      443      486      517   \n",
       "1    Topalak  43467  0.770833      404      442      451      483      514   \n",
       "2  Ayrik_Otu  43721  0.831250      383      409      446      507      495   \n",
       "3     Kanyas  43697  0.952083      408      431      447      471      502   \n",
       "4     Sirken  43629  0.012500      403      423      440      495      515   \n",
       "\n",
       "   510_570  530_590  ...  670_730  700_760  730_790  780_840  820_880  \\\n",
       "0      543      563  ...      690      734      789      829      839   \n",
       "1      526      552  ...      685      736      783      829      844   \n",
       "2      535      575  ...      708      722      749      785      875   \n",
       "3      513      545  ...      696      712      774      812      872   \n",
       "4      541      556  ...      699      735      743      805      853   \n",
       "\n",
       "   860_920  900_960  SICAKLIK  NEM  Fruit Numeric  \n",
       "0      888      939        24  163             10  \n",
       "1      897      939        20  176             10  \n",
       "2      916      941        20  169              0  \n",
       "3      838      932        25   96              5  \n",
       "4      889      954        26  107              9  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b35488f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "#Encode bitki_turu to Numeric\n",
    "label_encoder = LabelEncoder()\n",
    "df['Fruit Numeric'] = label_encoder.fit_transform(df['Bitki_Türü'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2a6f7fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "# Extract the features and labels\n",
    "X = df.drop([\"Bitki_Türü\",'Fruit Numeric','Tarih','Hour'], axis=1)\n",
    "y = df['Fruit Numeric']\n",
    "\n",
    "# Initialize the scalers\n",
    "min_max_scaler = MinMaxScaler()\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "# Scale the data using MinMaxScaler\n",
    "X_scaled_minmax = min_max_scaler.fit_transform(X)\n",
    "\n",
    "# Standardize the scaled data using StandardScaler\n",
    "X_scaled_std = std_scaler.fit_transform(X_scaled_minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53a4df06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled_std, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1a015dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c44998ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [8000,20,100,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e0c654ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mL_layer_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlayers_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprint_cost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [17], line 15\u001b[0m, in \u001b[0;36mL_layer_model\u001b[0;34m(X, Y, layers_dims, learning_rate, num_iterations, print_cost)\u001b[0m\n\u001b[1;32m     12\u001b[0m AL, caches \u001b[38;5;241m=\u001b[39m L_model_forward(X, parameters)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Compute cost.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m cost \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_cost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Backward propagation.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m grads \u001b[38;5;241m=\u001b[39m L_model_backward(AL, Y, caches)\n",
      "Cell \u001b[0;32mIn [9], line 2\u001b[0m, in \u001b[0;36mcompute_cost\u001b[0;34m(AL, Y, epsilon)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_cost\u001b[39m(AL, Y,epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-8\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     m \u001b[38;5;241m=\u001b[39m \u001b[43mY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      3\u001b[0m     cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mm) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(Y \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(AL \u001b[38;5;241m+\u001b[39m epsilon))\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cost\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "L_layer_model(X_train,y_train,layers_dims,print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5335e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e96bd88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0424019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d3cc74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03b79e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfe2482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cffea025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    AL, _ = L_model_forward(X, parameters)\n",
    "    Y_pred = np.argmax(AL, axis=0)\n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06409d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(Y_pred, Y):\n",
    "    num_correct = np.sum(Y_pred == Y)\n",
    "    print(num_correct)\n",
    "    accuracy = num_correct / Y.shape[0]\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d86608",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
