{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf99ff86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f95974a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a17908b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    #(≈ 4 lines of code)\n",
    "#     W1 = ...\n",
    "#     b1 = ...\n",
    "#     W2 = ...\n",
    "#     b2 = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h)* 0.01 \n",
    "    b2 = np.zeros((n_y, 1)) \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5506cd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters_deep\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # number of layers in the network\n",
    "    for l in range(1, L):\n",
    "        #(≈ 2 lines of code)\n",
    "#         parameters['W' + str(l)] = ...\n",
    "#         parameters['b' + str(l)] = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2642784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_forward\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    #(≈ 1 line of code)\n",
    "    # Z = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    Z=np.dot(W,A)+b\n",
    "    # YOUR CODE ENDS HERE\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "01064c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_forward\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        #(≈ 2 lines of code)\n",
    "#         Z, linear_cache = ...\n",
    "#         A, activation_cache = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        #(≈ 2 lines of code)\n",
    "#         Z, linear_cache = ...\n",
    "#         A, activation_cache = relu(Z)\n",
    "        # YOUR CODE STARTS HERE\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "275b0c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_forward\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- activation value from the output (last) layer\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "#     print(X,parameters)\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "#     print(parameters,'\\n\\n')\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    # The for loop starts at 1 because layer 0 is the input\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "#         print('['+ str(l) + ']' +' is =',  A_prev)\n",
    "        #(≈ 2 lines of code)\n",
    "#         A, cache = ...\n",
    "#         caches ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        A, cache = linear_activation_forward(A_prev,parameters[\"W\"+str(l)],parameters[\"b\"+str(l)],\"relu\")\n",
    "        caches.append(cache)\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    #(≈ 2 lines of code)\n",
    "#     AL, cache = ...\n",
    "#     caches ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "#     print(parameters[\"W\"+str(L)],parameters[\"b\"+str(L)])\n",
    "    AL, cache = linear_activation_forward(A,parameters[\"W\"+str(L)],parameters[\"b\"+str(L)],\"sigmoid\") \n",
    "    caches.append(cache)\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "          \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8f3b6ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    # (≈ 1 lines of code)\n",
    "    # cost = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    cost = - np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL))/m\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dfa05562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_backward\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "#     dW = ...\n",
    "#     db = ... sum by the rows of dZ with keepdims=True\n",
    "#     dA_prev = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    dW = 1/m*np.dot(dZ,A_prev.T)\n",
    "    db = 1/m*np.sum(dZ,axis=1,keepdims=True)  #sum by the rows of dZ with keepdims=True\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "942d3932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_backward\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        #(≈ 2 lines of code)\n",
    "#         dZ =  ...\n",
    "#         dA_prev, dW, db =  ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        dZ =  relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        #(≈ 2 lines of code)\n",
    "        # dZ =  ...\n",
    "        # dA_prev, dW, db =  ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        dZ = sigmoid_backward(dA, activation_cache) \n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bfcf2e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_backward\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    #(1 line of code)\n",
    "#     dAL = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    #(approx. 5 lines)\n",
    "    # current_cache = ...\n",
    "#     dA_prev_temp, dW_temp, db_temp = ...\n",
    "#     grads[\"dA\" + str(L-1)] = ...\n",
    "#     grads[\"dW\" + str(L)] = ...\n",
    "#     grads[\"db\" + str(L)] = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    current_cache = caches[L-1]\n",
    "    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache, 'sigmoid')\n",
    "    grads[\"dA\" + str(L-1)] = dA_prev_temp\n",
    "    grads[\"dW\" + str(L)] = dW_temp\n",
    "    grads[\"db\" + str(L)] = db_temp\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        #(approx. 5 lines)\n",
    "#         current_cache = ...\n",
    "#         dA_prev_temp, dW_temp, db_temp = ...\n",
    "#         grads[\"dA\" + str(l)] = ...\n",
    "#         grads[\"dW\" + str(l + 1)] = ...\n",
    "#         grads[\"db\" + str(l + 1)] = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dA_prev_temp, current_cache, 'relu')\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l+1)] = dW_temp\n",
    "        grads[\"db\" + str(l+1)] = db_temp\n",
    "        # YOUR CODE ENDS HERE\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "50c23b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(params, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    params -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    parameters = params.copy()\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    #(≈ 2 lines of code)\n",
    "    for l in range(L):\n",
    "#         parameters[\"W\" + str(l+1)] = ...\n",
    "#         parameters[\"b\" + str(l+1)] = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)]-learning_rate*grads['dW'+str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\"+str(l+1)]\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523fbea3",
   "metadata": {},
   "source": [
    "                                **Two layer Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "38f502e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: two_layer_model\n",
    "\n",
    "def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- If set to True, this will print the cost every 100 iterations \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary containing W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []                              # to keep track of the cost\n",
    "    m = X.shape[1]                           # number of examples\n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "    \n",
    "    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n",
    "    #(≈ 1 line of code)\n",
    "    # parameters = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1, W2, b2\". Output: \"A1, cache1, A2, cache2\".\n",
    "        #(≈ 2 lines of code)\n",
    "#         A1, cache1 = ...\n",
    "#         A2, cache2 = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1, 'relu')\n",
    "        A2, cache2 = linear_activation_forward(A1, W2, b2, 'sigmoid')\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        # Compute cost\n",
    "        #(≈ 1 line of code)\n",
    "        # cost = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        cost = compute_cost(A2, Y)\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        # Initializing backward propagation\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "        \n",
    "        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n",
    "        #(≈ 2 lines of code)\n",
    "#         dA1, dW2, db2 = ...\n",
    "#         dA0, dW1, db1 = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, 'sigmoid')\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, 'relu')\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        # Update parameters.\n",
    "        #(approx. 1 line of code)\n",
    "        # parameters = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "\n",
    "        # Retrieve W1, b1, W2, b2 from parameters\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        # Print the cost every 100 iterations\n",
    "        if print_cost and i % 100 == 0 or i == num_iterations - 1:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0 or i == num_iterations:\n",
    "            costs.append(cost)\n",
    "\n",
    "    return parameters, costs\n",
    "\n",
    "def plot_costs(costs, learning_rate=0.0075):\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6808e682",
   "metadata": {},
   "source": [
    "                                    N-Layer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "70ce95a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_layer_model\n",
    "\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    #(≈ 1 line of code)\n",
    "#     parameters = initialize_parameters_deep(layers_dims)\n",
    "    # YOUR CODE STARTS HERE\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        #(≈ 1 line of code)\n",
    "#         AL, caches = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        # Compute cost.\n",
    "        #(≈ 1 line of code)\n",
    "        # cost = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        cost = compute_cost(AL, Y)\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "        # Backward propagation.\n",
    "        #(≈ 1 line of code)\n",
    "#         grads = ...    \n",
    "        # YOUR CODE STARTS HERE\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    " \n",
    "        # Update parameters.\n",
    "        #(≈ 1 line of code)\n",
    "#         parameters = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "                \n",
    "        # Print the cost every 100 iterations\n",
    "        if print_cost and i % 100 == 0 or i == num_iterations - 1:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0 or i == num_iterations:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7804f0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.array([[12,90],[13,19],[32,23],[21,34],[23,98],[43,102],[29,234],[31,12],[33,344],[54,134],[67,123],[87,33],[15,66],[18,78]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d78bc411",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.array([237,99,183,161,297,385,593,157,829,493,523,423,201,237]).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fb5740b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims= [2,3,1]\n",
    "### CONSTANTS DEFINING THE MODEL ####\n",
    "# n_x = 2     # num_px * num_px * 3\n",
    "# n_h = 7\n",
    "# n_y = 1\n",
    "# layers_dims = (n_x, n_h, n_y)\n",
    "learning_rate = 0.0075"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e720e15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 12,  13,  32,  21,  23,  43,  29,  31,  33,  54,  67,  87,  15,\n",
       "         18],\n",
       "       [ 90,  19,  23,  34,  98, 102, 234,  12, 344, 134, 123,  33,  66,\n",
       "         78]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "de925db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[237,  99, 183, 161, 297, 385, 593, 157, 829, 493, 523, 423, 201,\n",
       "        237]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "555fef9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00050265 -0.00026102 -0.00055665 -0.00043359 -0.00069434 -0.00100477\n",
      "  -0.00127433 -0.00050212 -0.00173082 -0.00128314 -0.0014358  -0.00140673\n",
      "  -0.00046037 -0.00054811]] [[237]\n",
      " [ 99]\n",
      " [183]\n",
      " [161]\n",
      " [297]\n",
      " [385]\n",
      " [593]\n",
      " [157]\n",
      " [829]\n",
      " [493]\n",
      " [523]\n",
      " [423]\n",
      " [201]\n",
      " [237]]\n",
      "Cost after iteration 0: nan\n",
      "[[ -4820460.45564144  -1181094.69868615  -1681953.00533315\n",
      "   -2077443.50081837  -5403038.95209056  -5919420.89269479\n",
      "  -12497095.64546911  -1100178.90236132 -18221694.87144412\n",
      "   -7737454.95790306  -7373011.73033229  -3050526.02662981\n",
      "   -3631575.78875394  -4295874.47500517]] [[237]\n",
      " [ 99]\n",
      " [183]\n",
      " [161]\n",
      " [297]\n",
      " [385]\n",
      " [593]\n",
      " [157]\n",
      " [829]\n",
      " [493]\n",
      " [523]\n",
      " [423]\n",
      " [201]\n",
      " [237]]\n",
      "Cost after iteration 1: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wg/xx92xfmn7vq591t026kj8ztc0000gp/T/ipykernel_79641/2186034293.py:22: RuntimeWarning: invalid value encountered in log\n",
      "  cost = - np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL))/m\n",
      "/var/folders/wg/xx92xfmn7vq591t026kj8ztc0000gp/T/ipykernel_79641/4229360996.py:73: RuntimeWarning: overflow encountered in exp\n",
      "  s = 1/(1+np.exp(-Z))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters, costs = L_layer_model(train_x, train_y.T, layers_dims, num_iterations = 2, print_cost=True)\n",
    "plot_costs(costs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce9db0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925c324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, costs = L_layer_model(train_x, train_y, layers_dims, num_iterations = 5, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "49d39b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 15:38:18.891109: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lab_utils_multiclass_TF'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [43], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_blobs\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlab_utils_multiclass_TF\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lab_utils_multiclass_TF'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "from lab_utils_multiclass_TF import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0051538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make 4-class dataset for classification\n",
    "classes = 4\n",
    "m = 100\n",
    "centers = [[-5, 2], [-2, -2], [1, 2], [5, -2]]\n",
    "std = 1.0\n",
    "X_train, y_train = make_blobs(n_samples=m, centers=centers, cluster_std=std,random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d00119c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_mc(X_train,y_train,classes, centers, std=std)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
